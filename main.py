# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uDWK770htR4Fdr5lTBOq9Tyxllr7t_IK
"""

import os
import torch
import pandas as pd
from transformers import (
    AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments,
    DataCollatorForLanguageModeling, LineByLineTextDataset, set_seed
)

MODEL_PATH = "./sikubert"

BLOCK_SIZE = 128
MLM_PROB = 0.15
LR = 5e-5
WEIGHT_DECAY = 0.01
BATCH_SIZE = 8
EPOCHS = 3
SEED = 42
TOP_K = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_dataset(tokenizer, file_path):
    return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=BLOCK_SIZE)

def train(model, tokenizer, dataset, out_dir):
    set_seed(SEED)
    args = TrainingArguments(
        output_dir=out_dir,
        overwrite_output_dir=True,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        save_strategy="epoch",
        save_total_limit=1,
        learning_rate=LR,
        weight_decay=WEIGHT_DECAY,
        report_to="none",
        seed=SEED,
    )
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=dataset,
        data_collator=DataCollatorForLanguageModeling(
            tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB
        ),
        tokenizer=tokenizer,
    )
    trainer.train()
    trainer.save_model(out_dir)
    tokenizer.save_pretrained(out_dir)

def evaluate_top_k(model, tokenizer, file_path, k=TOP_K):
    model.eval()
    total_predictions = 0
    top_k_correct = 0
    top_1_correct = 0
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    for line in lines:
        inputs = tokenizer(line, return_tensors="pt").to(device)
        input_ids = inputs["input_ids"][0]
        for i in range(1, len(input_ids) - 1):
            original_token = input_ids[i].item()
            masked_input_ids = input_ids.clone()
            masked_input_ids[i] = tokenizer.mask_token_id
            with torch.no_grad():
                outputs = model(input_ids=masked_input_ids.unsqueeze(0))
                logits = outputs.logits
            probs = torch.softmax(logits[0, i], dim=-1)
            top_k_values, top_k_indices = torch.topk(probs, k)
            total_predictions += 1
            if original_token in top_k_indices:
                top_k_correct += 1
            if original_token == top_k_indices[0]:
                top_1_correct += 1
    top_k_accuracy = top_k_correct / total_predictions if total_predictions > 0 else 0
    top_1_accuracy = top_1_correct / total_predictions if total_predictions > 0 else 0
    return round(top_1_accuracy * 100, 2), round(top_k_accuracy * 100, 2)

def run_one_exp(train_file, test_file, name):
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH).to(device)
    dataset = load_dataset(tokenizer, train_file)
    train(model, tokenizer, dataset, f"./results/{name}")
    model = AutoModelForMaskedLM.from_pretrained(f"./results/{name}").to(device)
    tokenizer = AutoTokenizer.from_pretrained(f"./results/{name}")
    top1, topk = evaluate_top_k(model, tokenizer, test_file)
    print(f"[{name}] Top-1 Accuracy: {top1:.2f}%, Top-{TOP_K} Accuracy: {topk:.2f}%")
    return name, f"{top1:.2f}%", f"{topk:.2f}%"

if __name__ == "__main__":
    base_model_path = "./sikubert"

    for tag in ["inj", "rep"]:
        print(f"[{tag}] Running DAPT...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        model = AutoModelForMaskedLM.from_pretrained(base_model_path).to(device)
        dapt_dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path="./dapt.txt", block_size=BLOCK_SIZE)
        train(model, tokenizer, dapt_dataset, f"./results/{tag}_dapt")

        print(f"[{tag}] Running TAPT...")
        tokenizer = AutoTokenizer.from_pretrained(f"./results/{tag}_dapt")
        model = AutoModelForMaskedLM.from_pretrained(f"./results/{tag}_dapt").to(device)
        tapt_dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=f"train_{tag}.txt", block_size=BLOCK_SIZE)
        train(model, tokenizer, tapt_dataset, f"./results/{tag}_final")

        model = AutoModelForMaskedLM.from_pretrained(f"./results/{tag}_final").to(device)
        tokenizer = AutoTokenizer.from_pretrained(f"./results/{tag}_final")
        top1, topk = evaluate_top_k(model, tokenizer, f"test_{tag}.txt")
        print(f"[{tag}] Top-1 Accuracy: {top1:.2f}%, Top-{TOP_K} Accuracy: {topk:.2f}%")

import os
import torch
import pandas as pd
from transformers import (
    AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments,
    DataCollatorForLanguageModeling, LineByLineTextDataset, set_seed
)

MODEL_PATH = "bert-base-multilingual-cased"

BLOCK_SIZE = 128
MLM_PROB = 0.15
LR = 5e-5
WEIGHT_DECAY = 0.01
BATCH_SIZE = 8
EPOCHS = 3
SEED = 42
TOP_K = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_dataset(tokenizer, file_path):
    return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=BLOCK_SIZE)

def train(model, tokenizer, dataset, out_dir):
    set_seed(SEED)
    args = TrainingArguments(
        output_dir=out_dir,
        overwrite_output_dir=True,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        save_strategy="epoch",
        save_total_limit=1,
        learning_rate=LR,
        weight_decay=WEIGHT_DECAY,
        report_to="none",
        seed=SEED,
    )
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=dataset,
        data_collator=DataCollatorForLanguageModeling(
            tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB
        ),
        tokenizer=tokenizer,
    )
    trainer.train()
    trainer.save_model(out_dir)
    tokenizer.save_pretrained(out_dir)

def evaluate_top_k(model, tokenizer, file_path, k=TOP_K):
    model.eval()
    total_predictions = 0
    top_k_correct = 0
    top_1_correct = 0
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]
    for line in lines:
        inputs = tokenizer(line, return_tensors="pt").to(device)
        input_ids = inputs["input_ids"][0]
        for i in range(1, len(input_ids) - 1):
            original_token = input_ids[i].item()
            masked_input_ids = input_ids.clone()
            masked_input_ids[i] = tokenizer.mask_token_id
            with torch.no_grad():
                outputs = model(input_ids=masked_input_ids.unsqueeze(0))
                logits = outputs.logits
            probs = torch.softmax(logits[0, i], dim=-1)
            top_k_values, top_k_indices = torch.topk(probs, k)
            total_predictions += 1
            if original_token in top_k_indices:
                top_k_correct += 1
            if original_token == top_k_indices[0]:
                top_1_correct += 1
    top_k_accuracy = top_k_correct / total_predictions if total_predictions > 0 else 0
    top_1_accuracy = top_1_correct / total_predictions if total_predictions > 0 else 0
    return round(top_1_accuracy * 100, 2), round(top_k_accuracy * 100, 2)

def run_one_exp(train_file, test_file, name):
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH).to(device)
    dataset = load_dataset(tokenizer, train_file)
    train(model, tokenizer, dataset, f"./results/{name}")
    model = AutoModelForMaskedLM.from_pretrained(f"./results/{name}").to(device)
    tokenizer = AutoTokenizer.from_pretrained(f"./results/{name}")
    top1, topk = evaluate_top_k(model, tokenizer, test_file)
    print(f"[{name}] Top-1 Accuracy: {top1:.2f}%, Top-{TOP_K} Accuracy: {topk:.2f}%")
    return name, f"{top1:.2f}%", f"{topk:.2f}%"

if __name__ == "__main__":
    base_model_path = "./sikubert"

    for tag in ["inj", "rep"]:
        # Step 1: DAPT
        print(f"[{tag}] Running DAPT...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        model = AutoModelForMaskedLM.from_pretrained(base_model_path).to(device)
        dapt_dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path="./dapt.txt", block_size=BLOCK_SIZE)
        train(model, tokenizer, dapt_dataset, f"./results/{tag}_dapt")

        # Step 2: TAPT
        print(f"[{tag}] Running TAPT...")
        tokenizer = AutoTokenizer.from_pretrained(f"./results/{tag}_dapt")
        model = AutoModelForMaskedLM.from_pretrained(f"./results/{tag}_dapt").to(device)
        tapt_dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=f"train_{tag}.txt", block_size=BLOCK_SIZE)
        train(model, tokenizer, tapt_dataset, f"./results/{tag}_final")

        # Step 3: Evaluation
        model = AutoModelForMaskedLM.from_pretrained(f"./results/{tag}_final").to(device)
        tokenizer = AutoTokenizer.from_pretrained(f"./results/{tag}_final")
        top1, topk = evaluate_top_k(model, tokenizer, f"test_{tag}.txt")
        print(f"[{tag}] Top-1 Accuracy: {top1:.2f}%, Top-{TOP_K} Accuracy: {topk:.2f}%")

import torch
import math
import pandas as pd
import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForMaskedLM,
    LineByLineTextDataset,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

BLOCK_SIZE = 128
BATCH_SIZE = 8
SEED = 42
MLM_PROB = 0.15
REPEAT = 3

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def compute_ppl(model_path, eval_file, repeat=REPEAT):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForMaskedLM.from_pretrained(model_path).to(device)

    dataset = LineByLineTextDataset(
        tokenizer=tokenizer,
        file_path=eval_file,
        block_size=BLOCK_SIZE
    )

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB
    )

    args = TrainingArguments(
        output_dir="./tmp_eval",
        per_device_eval_batch_size=BATCH_SIZE,
        do_train=False,
        do_eval=True,
        report_to="none",
        seed=SEED
    )

    trainer = Trainer(
        model=model,
        args=args,
        data_collator=data_collator,
        eval_dataset=dataset,
    )

    ppl_list = []
    for _ in range(repeat):
        res = trainer.evaluate()
        loss = res["eval_loss"]
        ppl = math.exp(loss)
        ppl_list.append(ppl)

    mean = np.mean(ppl_list)
    std = np.std(ppl_list)
    return round(mean, 2), round(std, 2)

if __name__ == "__main__":
    tasks = [
        ("inj", "./mbert", "test_inj.txt"),
        ("inj", "./sikubert", "test_inj.txt"),
        ("rep", "./mbert", "test_rep.txt"),
        ("rep", "./sikubert", "test_rep.txt"),
    ]

    results = []
    for setting, model_path, test_file in tasks:
        mean_ppl, std_ppl = compute_ppl(model_path, test_file)
        print(f"[{setting}] {model_path} → PPL: {mean_ppl:.2f} ± {std_ppl:.2f}")
        results.append((setting, model_path, f"{mean_ppl:.2f} ± {std_ppl:.2f}"))

    df = pd.DataFrame(results, columns=["Setting", "Model", "PPL"])
    df.to_csv("ppl_eval_results.csv", index=False)
    print("\nSaved to ppl_eval_results.csv")